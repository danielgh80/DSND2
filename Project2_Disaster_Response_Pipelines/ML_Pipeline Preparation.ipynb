{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "\n",
    "connection = engine.raw_connection()\n",
    "table_name = str(engine.table_names()[0])\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM '{}'\".format(table_name), con=connection)\n",
    "\n",
    "cat = df.columns[4:]\n",
    "\n",
    "X = df['message'].values\n",
    "y = df[cat].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>related</td>\n",
       "      <td>20282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aid_related</td>\n",
       "      <td>10860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weather_related</td>\n",
       "      <td>7297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>direct_report</td>\n",
       "      <td>5075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>request</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>other_aid</td>\n",
       "      <td>3446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>food</td>\n",
       "      <td>2923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>earthquake</td>\n",
       "      <td>2455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>storm</td>\n",
       "      <td>2443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shelter</td>\n",
       "      <td>2314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>floods</td>\n",
       "      <td>2155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>medical_help</td>\n",
       "      <td>2084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>infrastructure_related</td>\n",
       "      <td>1705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>water</td>\n",
       "      <td>1672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>other_weather</td>\n",
       "      <td>1376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>buildings</td>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>medical_products</td>\n",
       "      <td>1313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>transport</td>\n",
       "      <td>1201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>death</td>\n",
       "      <td>1194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>other_infrastructure</td>\n",
       "      <td>1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>refugees</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>military</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>search_and_rescue</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>money</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>electricity</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cold</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>security</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>clothing</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>aid_centers</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>missing_people</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>hospitals</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>fire</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tools</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>shops</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>offer</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>child_alone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     index      0\n",
       "0                  related  20282\n",
       "1              aid_related  10860\n",
       "2          weather_related   7297\n",
       "3            direct_report   5075\n",
       "4                  request   4474\n",
       "5                other_aid   3446\n",
       "6                     food   2923\n",
       "7               earthquake   2455\n",
       "8                    storm   2443\n",
       "9                  shelter   2314\n",
       "10                  floods   2155\n",
       "11            medical_help   2084\n",
       "12  infrastructure_related   1705\n",
       "13                   water   1672\n",
       "14           other_weather   1376\n",
       "15               buildings   1333\n",
       "16        medical_products   1313\n",
       "17               transport   1201\n",
       "18                   death   1194\n",
       "19    other_infrastructure   1151\n",
       "20                refugees    875\n",
       "21                military    860\n",
       "22       search_and_rescue    724\n",
       "23                   money    604\n",
       "24             electricity    532\n",
       "25                    cold    530\n",
       "26                security    471\n",
       "27                clothing    405\n",
       "28             aid_centers    309\n",
       "29          missing_people    298\n",
       "30               hospitals    283\n",
       "31                    fire    282\n",
       "32                   tools    159\n",
       "33                   shops    120\n",
       "34                   offer    118\n",
       "35             child_alone      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,4:].sum().sort_values(ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['direct', 'news', 'social']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_counts = df.groupby('genre').count()['message']\n",
    "list(genre_counts.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "def tokenize(text, lemmatizer=WordNetLemmatizer()):\n",
    "    # Detect and replace URLs\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, 'urlplaceholder')\n",
    "    \n",
    "    # Remove all non-alpha-numeric characters and tokenize text\n",
    "    clean_tokens = nltk.word_tokenize(re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    clean_tokens = [t for t in clean_tokens if t not in stopwords.words('english')]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    clean_tokens = [lemmatizer.lemmatize(t) for t in clean_tokens]\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(class_weight='balanced')))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(y_true, y_pred):\n",
    "    for i in range(0, len(cat)):\n",
    "        print(cat[i])\n",
    "        print(\"\\tAccuracy: {:.4f}\\t Precision: {:.4f}\\t Recall: {:.4f}\\t F1_score: {:.4f}\".format(\n",
    "            accuracy_score(y_true[:, i], y_pred[:, i]),\n",
    "            precision_score(y_true[:, i], y_pred[:, i], average='weighted'),\n",
    "            recall_score(y_true[:, i], y_pred[:, i], average='weighted'),\n",
    "            f1_score(y_true[:, i], y_pred[:, i], average='weighted')\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "             criterion='gini', max_depth=None, max_features='auto',\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "             verbose=0, warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "             criterion='gini', max_depth=None, max_features='auto',\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "             verbose=0, warm_start=False),\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__class_weight': 'balanced',\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__n_jobs': 1,\n",
       " 'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x0000029CECBE1268>,\n",
       "           vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "               criterion='gini', max_depth=None, max_features='auto',\n",
       "               max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "               min_impurity_split=None, min_samples_leaf=1,\n",
       "               min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "               n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "               verbose=0, warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x0000029CECBE1268>,\n",
       "         vocabulary=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize>,\n",
       " 'vect__vocabulary': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "\tAccuracy: 0.8026\t Precision: 0.7925\t Recall: 0.8026\t F1_score: 0.7952\n",
      "request\n",
      "\tAccuracy: 0.8874\t Precision: 0.8788\t Recall: 0.8874\t F1_score: 0.8764\n",
      "offer\n",
      "\tAccuracy: 0.9950\t Precision: 0.9900\t Recall: 0.9950\t F1_score: 0.9925\n",
      "aid_related\n",
      "\tAccuracy: 0.7486\t Precision: 0.7465\t Recall: 0.7486\t F1_score: 0.7451\n",
      "medical_help\n",
      "\tAccuracy: 0.9255\t Precision: 0.9041\t Recall: 0.9255\t F1_score: 0.9004\n",
      "medical_products\n",
      "\tAccuracy: 0.9538\t Precision: 0.9465\t Recall: 0.9538\t F1_score: 0.9369\n",
      "search_and_rescue\n",
      "\tAccuracy: 0.9753\t Precision: 0.9759\t Recall: 0.9753\t F1_score: 0.9632\n",
      "security\n",
      "\tAccuracy: 0.9817\t Precision: 0.9640\t Recall: 0.9817\t F1_score: 0.9728\n",
      "military\n",
      "\tAccuracy: 0.9663\t Precision: 0.9556\t Recall: 0.9663\t F1_score: 0.9549\n",
      "child_alone\n",
      "\tAccuracy: 1.0000\t Precision: 1.0000\t Recall: 1.0000\t F1_score: 1.0000\n",
      "water\n",
      "\tAccuracy: 0.9536\t Precision: 0.9476\t Recall: 0.9536\t F1_score: 0.9439\n",
      "food\n",
      "\tAccuracy: 0.9284\t Precision: 0.9223\t Recall: 0.9284\t F1_score: 0.9178\n",
      "shelter\n",
      "\tAccuracy: 0.9281\t Precision: 0.9244\t Recall: 0.9281\t F1_score: 0.9071\n",
      "clothing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\danie\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.9852\t Precision: 0.9808\t Recall: 0.9852\t F1_score: 0.9791\n",
      "money\n",
      "\tAccuracy: 0.9785\t Precision: 0.9789\t Recall: 0.9785\t F1_score: 0.9684\n",
      "missing_people\n",
      "\tAccuracy: 0.9878\t Precision: 0.9760\t Recall: 0.9878\t F1_score: 0.9819\n",
      "refugees\n",
      "\tAccuracy: 0.9644\t Precision: 0.9512\t Recall: 0.9644\t F1_score: 0.9490\n",
      "death\n",
      "\tAccuracy: 0.9608\t Precision: 0.9578\t Recall: 0.9608\t F1_score: 0.9458\n",
      "other_aid\n",
      "\tAccuracy: 0.8699\t Precision: 0.8385\t Recall: 0.8699\t F1_score: 0.8225\n",
      "infrastructure_related\n",
      "\tAccuracy: 0.9326\t Precision: 0.8817\t Recall: 0.9326\t F1_score: 0.9022\n",
      "transport\n",
      "\tAccuracy: 0.9545\t Precision: 0.9433\t Recall: 0.9545\t F1_score: 0.9355\n",
      "buildings\n",
      "\tAccuracy: 0.9525\t Precision: 0.9411\t Recall: 0.9525\t F1_score: 0.9357\n",
      "electricity\n",
      "\tAccuracy: 0.9814\t Precision: 0.9727\t Recall: 0.9814\t F1_score: 0.9728\n",
      "tools\n",
      "\tAccuracy: 0.9939\t Precision: 0.9878\t Recall: 0.9939\t F1_score: 0.9909\n",
      "hospitals\n",
      "\tAccuracy: 0.9893\t Precision: 0.9788\t Recall: 0.9893\t F1_score: 0.9840\n",
      "shops\n",
      "\tAccuracy: 0.9965\t Precision: 0.9930\t Recall: 0.9965\t F1_score: 0.9947\n",
      "aid_centers\n",
      "\tAccuracy: 0.9875\t Precision: 0.9757\t Recall: 0.9875\t F1_score: 0.9816\n",
      "other_infrastructure\n",
      "\tAccuracy: 0.9541\t Precision: 0.9111\t Recall: 0.9541\t F1_score: 0.9321\n",
      "weather_related\n",
      "\tAccuracy: 0.8633\t Precision: 0.8607\t Recall: 0.8633\t F1_score: 0.8567\n",
      "floods\n",
      "\tAccuracy: 0.9348\t Precision: 0.9312\t Recall: 0.9348\t F1_score: 0.9188\n",
      "storm\n",
      "\tAccuracy: 0.9327\t Precision: 0.9232\t Recall: 0.9327\t F1_score: 0.9206\n",
      "fire\n",
      "\tAccuracy: 0.9899\t Precision: 0.9854\t Recall: 0.9899\t F1_score: 0.9858\n",
      "earthquake\n",
      "\tAccuracy: 0.9631\t Precision: 0.9615\t Recall: 0.9631\t F1_score: 0.9612\n",
      "cold\n",
      "\tAccuracy: 0.9809\t Precision: 0.9758\t Recall: 0.9809\t F1_score: 0.9736\n",
      "other_weather\n",
      "\tAccuracy: 0.9504\t Precision: 0.9356\t Recall: 0.9504\t F1_score: 0.9283\n",
      "direct_report\n",
      "\tAccuracy: 0.8541\t Precision: 0.8408\t Recall: 0.8541\t F1_score: 0.8352\n"
     ]
    }
   ],
   "source": [
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using all CPU cores, otherwise it takes forever...\n",
    "parameters = {\n",
    "    'vect__min_df': [1, 10],\n",
    "    'vect__lowercase': [True, False],\n",
    "    'tfidf__smooth_idf': [True, False],\n",
    "    'clf__estimator__min_samples_split': [2, 5],\n",
    "    'clf__estimator__n_estimators': [10, 20] \n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, cv = 2, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "             criterion='gini', max_depth=None, max_features='auto',\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "             verbose=0, warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "             criterion='gini', max_depth=None, max_features='auto',\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "             verbose=0, warm_start=False),\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__class_weight': 'balanced',\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__n_jobs': 1,\n",
       " 'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x0000029CECBE1268>,\n",
       "           vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "               criterion='gini', max_depth=None, max_features='auto',\n",
       "               max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "               min_impurity_split=None, min_samples_leaf=1,\n",
       "               min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "               n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "               verbose=0, warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x0000029CECBE1268>,\n",
       "         vocabulary=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize>,\n",
       " 'vect__vocabulary': None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__min_df': [1, 10], 'vect__lowercase': [True, False], 'tfidf__smooth_idf': [True, False], 'clf__estimator__min_samples_split': [2, 5], 'clf__estimator__n_estimators': [10, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "\tAccuracy: 0.8164\t Precision: 0.8106\t Recall: 0.8164\t F1_score: 0.8113\n",
      "request\n",
      "\tAccuracy: 0.8897\t Precision: 0.8847\t Recall: 0.8897\t F1_score: 0.8865\n",
      "offer\n",
      "\tAccuracy: 0.9945\t Precision: 0.9890\t Recall: 0.9945\t F1_score: 0.9918\n",
      "aid_related\n",
      "\tAccuracy: 0.7653\t Precision: 0.7670\t Recall: 0.7653\t F1_score: 0.7659\n",
      "medical_help\n",
      "\tAccuracy: 0.9278\t Precision: 0.9137\t Recall: 0.9278\t F1_score: 0.9162\n",
      "medical_products\n",
      "\tAccuracy: 0.9556\t Precision: 0.9438\t Recall: 0.9556\t F1_score: 0.9438\n",
      "search_and_rescue\n",
      "\tAccuracy: 0.9765\t Precision: 0.9702\t Recall: 0.9765\t F1_score: 0.9663\n",
      "security\n",
      "\tAccuracy: 0.9831\t Precision: 0.9713\t Recall: 0.9831\t F1_score: 0.9753\n",
      "military\n",
      "\tAccuracy: 0.9681\t Precision: 0.9602\t Recall: 0.9681\t F1_score: 0.9622\n",
      "child_alone\n",
      "\tAccuracy: 1.0000\t Precision: 1.0000\t Recall: 1.0000\t F1_score: 1.0000\n",
      "water\n",
      "\tAccuracy: 0.9596\t Precision: 0.9560\t Recall: 0.9596\t F1_score: 0.9567\n",
      "food\n",
      "\tAccuracy: 0.9385\t Precision: 0.9347\t Recall: 0.9385\t F1_score: 0.9352\n",
      "shelter\n",
      "\tAccuracy: 0.9365\t Precision: 0.9288\t Recall: 0.9365\t F1_score: 0.9291\n",
      "clothing\n",
      "\tAccuracy: 0.9847\t Precision: 0.9805\t Recall: 0.9847\t F1_score: 0.9809\n",
      "money\n",
      "\tAccuracy: 0.9800\t Precision: 0.9738\t Recall: 0.9800\t F1_score: 0.9726\n",
      "missing_people\n",
      "\tAccuracy: 0.9884\t Precision: 0.9849\t Recall: 0.9884\t F1_score: 0.9835\n",
      "refugees\n",
      "\tAccuracy: 0.9695\t Precision: 0.9595\t Recall: 0.9695\t F1_score: 0.9600\n",
      "death\n",
      "\tAccuracy: 0.9641\t Precision: 0.9581\t Recall: 0.9641\t F1_score: 0.9555\n",
      "other_aid\n",
      "\tAccuracy: 0.8712\t Precision: 0.8405\t Recall: 0.8712\t F1_score: 0.8405\n",
      "infrastructure_related\n",
      "\tAccuracy: 0.9394\t Precision: 0.9104\t Recall: 0.9394\t F1_score: 0.9138\n",
      "transport\n",
      "\tAccuracy: 0.9562\t Precision: 0.9426\t Recall: 0.9562\t F1_score: 0.9423\n",
      "buildings\n",
      "\tAccuracy: 0.9554\t Precision: 0.9458\t Recall: 0.9554\t F1_score: 0.9472\n",
      "electricity\n",
      "\tAccuracy: 0.9786\t Precision: 0.9698\t Recall: 0.9786\t F1_score: 0.9707\n",
      "tools\n",
      "\tAccuracy: 0.9948\t Precision: 0.9897\t Recall: 0.9948\t F1_score: 0.9922\n",
      "hospitals\n",
      "\tAccuracy: 0.9892\t Precision: 0.9785\t Recall: 0.9892\t F1_score: 0.9838\n",
      "shops\n",
      "\tAccuracy: 0.9947\t Precision: 0.9893\t Recall: 0.9947\t F1_score: 0.9920\n",
      "aid_centers\n",
      "\tAccuracy: 0.9898\t Precision: 0.9800\t Recall: 0.9898\t F1_score: 0.9848\n",
      "other_infrastructure\n",
      "\tAccuracy: 0.9597\t Precision: 0.9309\t Recall: 0.9597\t F1_score: 0.9415\n",
      "weather_related\n",
      "\tAccuracy: 0.8727\t Precision: 0.8712\t Recall: 0.8727\t F1_score: 0.8718\n",
      "floods\n",
      "\tAccuracy: 0.9460\t Precision: 0.9402\t Recall: 0.9460\t F1_score: 0.9389\n",
      "storm\n",
      "\tAccuracy: 0.9359\t Precision: 0.9314\t Recall: 0.9359\t F1_score: 0.9330\n",
      "fire\n",
      "\tAccuracy: 0.9889\t Precision: 0.9837\t Recall: 0.9889\t F1_score: 0.9839\n",
      "earthquake\n",
      "\tAccuracy: 0.9628\t Precision: 0.9610\t Recall: 0.9628\t F1_score: 0.9611\n",
      "cold\n",
      "\tAccuracy: 0.9850\t Precision: 0.9816\t Recall: 0.9850\t F1_score: 0.9795\n",
      "other_weather\n",
      "\tAccuracy: 0.9468\t Precision: 0.9304\t Recall: 0.9468\t F1_score: 0.9303\n",
      "direct_report\n",
      "\tAccuracy: 0.8517\t Precision: 0.8446\t Recall: 0.8517\t F1_score: 0.8473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dl/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ada_parameters = {\n",
    "    'vect__min_df': [1, 20, 50, 100],\n",
    "    'vect__lowercase': [True, False],\n",
    "    'tfidf__smooth_idf': [True, False],\n",
    "    'tfidf__sublinear_tf': [True, False],\n",
    "    'clf__estimator__algorithm': ['SAMME', 'SAMME.R'],\n",
    "    'clf__estimator__n_estimators': [1, 5, 10, 50] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using all CPU cores, otherwise it takes forever...\n",
    "ada_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(AdaBoostClassifier(random_state=10)))\n",
    "])\n",
    "\n",
    "ada_cv = GridSearchCV(ada_pipeline, param_grid = ada_parameters, cv = 2, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7fe97f5aba60>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "             learning_rate=1.0, n_estimators=50, random_state=10),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7fe97f5aba60>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=10),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text, lemmatizer=<WordNetLemmatizer>)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__algorithm': 'SAMME.R',\n",
       " 'clf__estimator__base_estimator': None,\n",
       " 'clf__estimator__learning_rate': 1.0,\n",
       " 'clf__estimator__n_estimators': 50,\n",
       " 'clf__estimator__random_state': 10,\n",
       " 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=10),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...timator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=10),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__min_df': [1, 20, 50, 100], 'vect__lowercase': [True, False], 'tfidf__smooth_idf': [True, False], 'tfidf__sublinear_tf': [True, False], 'clf__estimator__algorithm': ['SAMME', 'SAMME.R'], 'clf__estimator__n_estimators': [1, 5, 10, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = ada_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "\tAccuracy: 0.7864\t Precision: 0.7569\t Recall: 0.7864\t F1_score: 0.7431\n",
      "request\n",
      "\tAccuracy: 0.8834\t Precision: 0.8744\t Recall: 0.8834\t F1_score: 0.8746\n",
      "offer\n",
      "\tAccuracy: 0.9940\t Precision: 0.9909\t Recall: 0.9940\t F1_score: 0.9920\n",
      "aid_related\n",
      "\tAccuracy: 0.7611\t Precision: 0.7621\t Recall: 0.7611\t F1_score: 0.7557\n",
      "medical_help\n",
      "\tAccuracy: 0.9318\t Precision: 0.9193\t Recall: 0.9318\t F1_score: 0.9190\n",
      "medical_products\n",
      "\tAccuracy: 0.9603\t Precision: 0.9530\t Recall: 0.9603\t F1_score: 0.9537\n",
      "search_and_rescue\n",
      "\tAccuracy: 0.9774\t Precision: 0.9710\t Recall: 0.9774\t F1_score: 0.9708\n",
      "security\n",
      "\tAccuracy: 0.9823\t Precision: 0.9720\t Recall: 0.9823\t F1_score: 0.9757\n",
      "military\n",
      "\tAccuracy: 0.9687\t Precision: 0.9618\t Recall: 0.9687\t F1_score: 0.9637\n",
      "child_alone\n",
      "\tAccuracy: 1.0000\t Precision: 1.0000\t Recall: 1.0000\t F1_score: 1.0000\n",
      "water\n",
      "\tAccuracy: 0.9587\t Precision: 0.9555\t Recall: 0.9587\t F1_score: 0.9565\n",
      "food\n",
      "\tAccuracy: 0.9434\t Precision: 0.9411\t Recall: 0.9434\t F1_score: 0.9418\n",
      "shelter\n",
      "\tAccuracy: 0.9442\t Precision: 0.9389\t Recall: 0.9442\t F1_score: 0.9394\n",
      "clothing\n",
      "\tAccuracy: 0.9863\t Precision: 0.9834\t Recall: 0.9863\t F1_score: 0.9835\n",
      "money\n",
      "\tAccuracy: 0.9808\t Precision: 0.9763\t Recall: 0.9808\t F1_score: 0.9774\n",
      "missing_people\n",
      "\tAccuracy: 0.9896\t Precision: 0.9877\t Recall: 0.9896\t F1_score: 0.9864\n",
      "refugees\n",
      "\tAccuracy: 0.9706\t Precision: 0.9632\t Recall: 0.9706\t F1_score: 0.9646\n",
      "death\n",
      "\tAccuracy: 0.9678\t Precision: 0.9631\t Recall: 0.9678\t F1_score: 0.9634\n",
      "other_aid\n",
      "\tAccuracy: 0.8682\t Precision: 0.8319\t Recall: 0.8682\t F1_score: 0.8323\n",
      "infrastructure_related\n",
      "\tAccuracy: 0.9379\t Precision: 0.9148\t Recall: 0.9379\t F1_score: 0.9202\n",
      "transport\n",
      "\tAccuracy: 0.9588\t Precision: 0.9490\t Recall: 0.9588\t F1_score: 0.9477\n",
      "buildings\n",
      "\tAccuracy: 0.9623\t Precision: 0.9568\t Recall: 0.9623\t F1_score: 0.9576\n",
      "electricity\n",
      "\tAccuracy: 0.9811\t Precision: 0.9767\t Recall: 0.9811\t F1_score: 0.9772\n",
      "tools\n",
      "\tAccuracy: 0.9944\t Precision: 0.9914\t Recall: 0.9944\t F1_score: 0.9925\n",
      "hospitals\n",
      "\tAccuracy: 0.9872\t Precision: 0.9815\t Recall: 0.9872\t F1_score: 0.9839\n",
      "shops\n",
      "\tAccuracy: 0.9944\t Precision: 0.9893\t Recall: 0.9944\t F1_score: 0.9918\n",
      "aid_centers\n",
      "\tAccuracy: 0.9881\t Precision: 0.9826\t Recall: 0.9881\t F1_score: 0.9849\n",
      "other_infrastructure\n",
      "\tAccuracy: 0.9565\t Precision: 0.9375\t Recall: 0.9565\t F1_score: 0.9443\n",
      "weather_related\n",
      "\tAccuracy: 0.8691\t Precision: 0.8685\t Recall: 0.8691\t F1_score: 0.8621\n",
      "floods\n",
      "\tAccuracy: 0.9570\t Precision: 0.9539\t Recall: 0.9570\t F1_score: 0.9528\n",
      "storm\n",
      "\tAccuracy: 0.9361\t Precision: 0.9295\t Recall: 0.9361\t F1_score: 0.9302\n",
      "fire\n",
      "\tAccuracy: 0.9890\t Precision: 0.9853\t Recall: 0.9890\t F1_score: 0.9860\n",
      "earthquake\n",
      "\tAccuracy: 0.9715\t Precision: 0.9705\t Recall: 0.9715\t F1_score: 0.9705\n",
      "cold\n",
      "\tAccuracy: 0.9854\t Precision: 0.9818\t Recall: 0.9854\t F1_score: 0.9825\n",
      "other_weather\n",
      "\tAccuracy: 0.9460\t Precision: 0.9280\t Recall: 0.9460\t F1_score: 0.9286\n",
      "direct_report\n",
      "\tAccuracy: 0.8506\t Precision: 0.8368\t Recall: 0.8506\t F1_score: 0.8322\n"
     ]
    }
   ],
   "source": [
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('classifier.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(cv, pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
